[{"title":"利用hexo + github pages构建个人博客技术总结","date":"2024-04-20T02:23:03.000Z","url":"/blog_share/","categories":[["undefined",""]]},{"title":"论文阅读 | ImageReward","date":"2024-04-20T02:23:03.000Z","url":"/ImageReward/","categories":[["undefined",""]],"content":"论文标题:ImageReward:Learning and Evaluating Human Preferences for Text-to-Image Generation发表年份：2023会议/期刊：NeurIPS论文链接： ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation1、引言 最近几年Text-to-image生成模型发展迅速，包括自回归模型（主要是一系列基于Transformer架构的模型例如DALLE、CogView、Parti等）和扩散模型（基于Diffusion架构的模型例如DALLE-2、Stable Diffusion等）。这些模型能生成具有很高保真度和很强语义相关性的图像。 尽管如此，现在这些自监督模型离完美还很远。一个主要的挑战在于将模型与人类偏好对齐，因为预训练分布是有噪声的，并且与实际的用户提示分布不同。还存在一些问题，包括但不限于： 图文一致性：生成的图像未能准确描述所有的数字、属性和⽂字提⽰中描述的对象关系。（例如上图中的a, b，“小女孩被地上的向日葵挡住去路”以及“太阳神加冕”） 肢体问题：生成的图像呈现了扭曲、不完整、重复或者异常的肢体部位（例如：四肢等），这一问题在人类或动物中均可能出现。（例如上图中的e和f） 审美问题：生成的图像偏离⼈类对审美⻛格的平均或主流偏好。（例如上图中的c和d） 有害与偏⻅内容：生成的图像具有有害、暴⼒、性相关、存在歧视、⾮法或引起⼼理不适的内容。（例如上图中的f） 这些问题是很难通过改进模型结构和预训练数据去解决的。 在NLP中研究人员采用来自人类反馈的强化学习（RLHF）去引导大语言模型靠近人类的偏好和价值观。这种方法依靠训练一个奖励模型（RM）来从大量专家注释的模型输出比较中捕获人类偏好。虽然这个方法是有效的，但是成本非常高，专家注释是非常困难的。 主要贡献我们系统地识别了文本到图像的人类偏好标注面临的挑战，并因此设计了一个专门针对它的管道，建立了定量评估和标注员培训的标准，优化了标注体验，并确保了质量验证。我们基于管道构建了用于训练ImageReward模型的文本-图像比较数据集。整个体系结构如图2所示。 •通过广泛的分析和实验，我们证明ImageReward在理解人类在文本到图像合成中的偏好方面优于现有的文本图像评分方法，如CLIP41、Aesthetic50和BLIP26。ImageReward也被证明可以显著缓解上述问题，为如何将人类偏好集成到生成模型中提供了有价值的见解。 •我们建议ImageReward可以作为一个有前途的自动文本到图像的评估指标。与FID[18]和CLIP在真实用户和MS-COCO 2014提示上的得分相比，ImageReward始终与人类偏好排名保持一致，并在模型和样本之间表现出更高的可区分性。 提出人类奖励反馈学习（ReFL：Reward Feedback Learning）来调整关于人类偏好评分者的扩散模型。ImageReward在后期去噪步骤中的质量可识别性的独特见解允许对扩散模型进行直接反馈学习，而扩散模型不提供其生成的likelihood。广泛的自动和人工评估表明，ReFL优于现有的方法，包括数据增强[61;[13]损失重估b[23]。 2 ImageReward: Learning to Score and Evaluate Human Preferences标注设计流程1、Prompt Selection and Image Collection：数据集来自一个开源的数据集 DiffusionDB 上的各种真实用户提示。 通过算法产生10k个候选prompt，每个prompt都有4-9张来自DiffusionDB的生成图片。从而产生177304个用于标记的候选对。 2、Human Annotation Design：三个阶段：Prompt Annotation 对prompt进行分类和识别是否有问题。Text-Image Rating 从文本对齐，保真度和是否有害三个方面对图像进行打分。 Image Ranking 按偏好顺序给图像排序 他们给数据打标的页面 3、Human Annotation Analysis：经过2个月的注释，收集了8,878个提示的有效注释，产生了136,892对比较 RW Training 人类偏好预测模型训练有$k \\in [4,9]$张图片对相同的提示$T$进行排序，从最好到最差标记为$x_1,x_2,…,x_k$，如果两个图像之间没有联系，则最多可以产生$C_k^2$个比较对。 对于每一个比较对，如果$x_i$是较好的，$x_j$是较差的，那损失函数可以表示为： loss(\\theta)=-\\mathbb{E}_{(T,x_i,x_j) \\sim \\mathcal{D}}[log(\\sigma (f_{\\theta}(T,x_i)-f_{\\theta}(T,x_j)))]其中$f_{\\theta}(T,x)$就是人类偏好打分模型 ImageReward的结构：使用BLIP作为骨干网络。用BLIP提取图像和文本特征之后，使用cross attention把特征联合起来。最后使用一个MLP进行打分。 训练的时候会快速收敛并且会过拟合，作者就把一些骨干网络transformer层的参数固定。ImageReward对训练超参数很敏感，例如学习率和batchsize，所以作者在验证集执行网格搜索。 As Metric: Re-Evaluating Human Preferences on Text-to-Image Models 使用ImageReward作为自动评测指标现在评估生成模型主要有两个指标FID和CLIP Score是两种流行的评估方法。 FID（fine-tune或者zero-shot）：使用MS-COCO image caption数据集，文本到图像生成模型与真实图像进行评估。 CLIP Score：使用CLIP来衡量模型生成的图片和文本之间的匹配度。 FID有两个问题： Zero-shot Usage：由于生成模型现在主要被公众以没有微调的零样本方式使用，微调后的FID可能不会真实地反映模型在实际使用中的实际性能。此外，尽管在最近的流行做法中采用了零样本FID，一些模型的预训练数据中MS-COCO的可能泄漏将使其成为一个潜在的不公平设置。 Human Preference：FID衡量生成的图像和参考真实图像之间的平均距离，因此在评估中未能包含对文本到图像合成至关重要的人类偏好。此外，FID依赖于整个数据集的平均值来提供准确的评估，而在许多情况下，我们需要该指标作为单个图像的选择器。 使用ImageReward作为一个zero-shot的自动评估标准，用于文本到图像的比较和个体样本的选择。 Better Human Alignment Across Models： 对6个流行的高分辨率(约512 × 512)可用的文本到图像模型进行了评测：CogView 2、Versatile Diffusion (VD)、Stable Diffusion (SD) 1.4和2.1、DALL-E 2(通过OpenAI API)和Openjourney。测试所用的100个prompt从真实用户使用prompt中采样得到，每个模型根据每个prompt生成10个图片作为候选。为了比较这些模型，我们首先从每个模型对每个prompt的10个输出中选择最佳的图像。然后，标注员根据ImageRewardDB中描述的排序规则，对每个prompt下来自不同模型的图像进行排序。每个模型对所有其他模型的最终获胜计数见上表。如表所示：ImageReward与人类排名很一致，而零样本的FID和CLIP则不一致。 Better Distinguishability Across Models and Samples 与CLIP相比，我们观察到ImageReward可以更好地区分单个样本的质量。在分数已经归一化的情况下，ImageReward在每个模型上的得分比CLIP有更大的差别，就是说ImageReward有更大的区分度，CLIP的得分只在这个小区间，区分度不高。并且ImageReward得分的中位数，和表一的排名一致。 3、ReFL：Reward Feedback Learning Improves Text-to-Image虽然ImageReward可以很好地挑选出满足人类偏好地图片，但是在实际应用中，“生成-然后过滤”这种范式是昂贵且低效的。 Challenge：在NLP领域中已经有使用强化学习算法图引导语言模型与人类偏好保持一致。这种引导依赖于语言模型生成的likelihood（likelihood就是概率，就是语言模型每一次生成一个单词的那个概率列表）来更新整个模型。 但是在latent diffusion models中，不像语言模型，扩散模型在多个扩散去噪生成是不会生成likelihood的，所以不能采用相同的RLHF方法。 ReFL设计： 通过观察去噪步骤中的ImageReward分数，我们得出了一个有趣的发现（参见上图左）。对于一个降噪过程，例如降噪步数为40步时，在降噪过程中途直接预测中间降噪结果对应的原图： 当t ≤ 15：ImageReward得分和最终结果的一致性很低； 当15 ≤ t ≤ 30：高质量生成结果的ImageReward得分开始脱颖而出，但总体上我们仍然无法根据目前的ImageReward分数清楚地判断所有生成结果的最终质量； 当t ≥ 30:不同生成结果对应的ImageReward分数的已经可以区分。 根据观察，我们得出结论，经过30步去噪（总步数为40步），而不需要到最后一步降噪，ImageReward分数可以作为改进LDM的可靠反馈。因此，我们提出了一种直接微调LDM的算法。算法流程可见上图右。将RM的分数视为人类的偏好损失，将梯度反向传播到去噪过程中随机挑选的后一步t（在我们的例子中t取值范围为30~40）。随机选择t而不是使用最后一步的原因是，如果只保留最后一个去噪步骤的梯度，训练被证明是非常不稳定的，结果是不好的。在实践中，为了避免快速过拟合和稳定微调，我们对ReFL Loss进行重新加权，并用Pre-training Loss进行正则化。 作者提出一个算法，通过将RW的分数视为人类偏好损失来直接微调LDMs。就是通过反向传播把梯度传到去噪过程中随机选择的后一步$t$（作者例子$t\\in [30,40]$）。 随机选择t而不是使用最后一步的原因是，如果只保留最后一个去噪步骤的梯度，训练被证明是非常不稳定的，结果是不好的。在实践中，为了避免快速过拟合和稳定微调，我们对ReFL Loss进行重新加权，并用Pre-training Loss进行正则化。 最终的loss为： \\mathcal{L}_{reward} = \\lambda \\mathbb{E}_{y_i \\sim \\mathcal{Y}}(\\phi(r(y_i,g_{\\theta}(y_i)))) \\\\ \\mathcal{L}_{pre} = \\mathbb{E}_{(y_i,x_i) \\sim \\mathcal{D}}(\\mathbb{E}_{\\varepsilon(x_i),y_i,\\epsilon\\sim\\mathcal{N}(0,1),t}[\\| \\epsilon - \\epsilon_{\\theta}(z_t,t,\\tau_{\\theta}(y_i)) \\|_2^2])$\\theta$是LDM的参数，$g_{\\theta}$是LDM用prompt$y_i$生成的图片 每一次迭代可以看成两个步骤。数据集$D$其实是LAION的子集，提示集是DiffusionDB里面选的prompt。 第一个步骤就是第六，七行。这就是使用$D$去训练diffusion，就是传统的diffusion训练过程。 第二个步骤可以看作是一个生成过程。先从一个随机高斯噪声去噪。作者这里$T=40,[T_1,T_2]=[1,10]$ 。去噪的前面步骤都不需要梯度。在最后的第t步时才计算梯度.并且在这时就生成图片，用ImageReward打分，然后计算损失再次更新LDM的参数。 我的理解就是第一步优化就是提升模型性能，第二步就是让模型靠近人类偏好。有点像MAAT框架，或许可以分开来训练 实验结果 们的实验基于Stable Diffusion v1.4，奖励模型统一采用ImageReward，使用PNDM作为noise scheduler，classifier free guidance采用默认值7.5。相比于微调之前的Stable Diffusion v1.4，ReFL微调模型最受青睐而且胜率最高。当和其他方法相互比较时，ReFL的结果总是更受偏好 总结在这项工作中，我们提出了ImageReward和ReFL，这是第一个通用的文本到图像的人类偏好奖励模型，以及通过ImageReward反馈优化扩散模型的直接微调方法。通过我们系统的人类偏好标注管道，我们收集了一个137k的专家比较数据集来训练ImageReward，并在此基础上构建ReFL算法"},{"title":"About Me","date":"2024-04-17T16:06:04.000Z","url":"/about/","categories":[["undefined",""]],"content":"个人信息华东师范大学数据学院的硕士研究生 研究方向AIGC Diffusion 爱好无"}]